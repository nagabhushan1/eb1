{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b61100ab",
   "metadata": {},
   "source": [
    "# ðŸ§© Team_1_CODE_FLUX\n",
    "\n",
    "## ðŸ“˜ Project Title\n",
    "**Rental Data Pipeline**\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "To design and implement an end-to-end data pipeline using Amazon S3, AWS EMR (PySpark), AWS Glue Crawlers, Amazon Athena, and AWS Step Functions that ingests, processes, and analyzes rental vehicle data, enabling data-driven insights for business optimization.\n",
    "\n",
    "## ðŸ§  Tech Stack\n",
    "Amazon S3 Â· AWS EMR (PySpark) Â· AWS Glue Crawlers Â· Amazon Athena Â· AWS Step Functions\n",
    "\n",
    "## ðŸ‘¥ Team Members\n",
    "- Vijay Kumar E\n",
    "- Smita Sudhakar Hegde\n",
    "- Sakshath K Shetty\n",
    "- Mallika Shree K C\n",
    "- Gokul Raj S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fddbeee",
   "metadata": {},
   "source": [
    "## ðŸ§© Project Modules & Backlogs\n",
    "### Module 1: Data Ingestion\n",
    "- Upload raw CSV files (locations, users, vehicles, transactions) to an S3 bucket.\n",
    "- Set up folder structure for raw, processed, and analytics data.\n",
    "- Ensure files are accessible and permissions are correctly applied.\n",
    "\n",
    "### Module 2: Data Processing\n",
    "- Create EMR cluster for running PySpark jobs.\n",
    "- Write and run PySpark scripts for data cleaning and basic aggregations.\n",
    "- Store processed output back to S3 in structured format (CSV or Parquet).\n",
    "\n",
    "### Module 3: Workflow Control\n",
    "- Use AWS Step Functions to manage job sequence for data ingestion and processing.\n",
    "- Add simple success/failure states and basic logging.\n",
    "- Test workflow execution with sample data.\n",
    "\n",
    "### Module 4: Access & Permissions\n",
    "- Attach correct IAM roles for EMR and Step Functions using provided policy files.\n",
    "- Verify permissions for reading and writing to S3.\n",
    "- Ensure minimum privileges are applied.\n",
    "\n",
    "### Module 5: Query & Reporting\n",
    "- Set up Athena database and tables for processed data.\n",
    "- Run sample queries to check record counts and summaries.\n",
    "- Prepare a simple report or chart showing top-performing vehicles or locations.\n",
    "\n",
    "### Module 6: Validation & Testing\n",
    "- Use provided local script for basic testing and validation.\n",
    "- Check schema consistency and data accuracy.\n",
    "- Confirm end-to-end data flow works as expected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfa95ae",
   "metadata": {},
   "source": [
    "## ðŸš€ Sprint 1 â€“ AWS Setup & Initialization\n",
    "**Dates:** 16â€“18 Oct 2025 (3 days)\n",
    "\n",
    "**Goal:** Get all AWS services and datasets ready for pipeline development.\n",
    "\n",
    "**Tasks:**\n",
    "- Create S3 bucket and folder structure for raw and processed data.\n",
    "- Upload sample datasets (CSV files) into the raw folder.\n",
    "- Configure IAM roles and attach trust and execution policies.\n",
    "- Test connectivity between S3, EMR, and Step Functions.\n",
    "- Validate permissions and access policies.\n",
    "\n",
    "**Deliverable:**\n",
    "AWS environment and dataset ready for development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018ed491",
   "metadata": {},
   "source": [
    "## ðŸš€ Sprint 2 â€“ Pipeline Development & Execution\n",
    "**Dates:** 23â€“26 Oct 2025 (4 days)\n",
    "\n",
    "**Goal:** Build and run the PySpark-based data pipeline using EMR.\n",
    "\n",
    "**Tasks:**\n",
    "- Write PySpark scripts for data transformation and aggregation.\n",
    "- Deploy PySpark jobs on EMR and validate job execution.\n",
    "- Connect EMR output to Athena for query access.\n",
    "- Test Step Functions flow with EMR job trigger.\n",
    "- Ensure transformed data is available in S3 for analysis.\n",
    "\n",
    "**Deliverable:**\n",
    "Working data pipeline that processes and stores clean, structured data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae7b27e",
   "metadata": {},
   "source": [
    "## ðŸš€ Sprint 3 â€“ Integration, Testing & Demo\n",
    "**Dates:** 27â€“31 Oct 2025 (4 days)\n",
    "\n",
    "**Goal:** Combine all modules, test end-to-end flow, and prepare for final presentation.\n",
    "\n",
    "**Tasks:**\n",
    "- Run the entire pipeline from ingestion to reporting.\n",
    "- Execute Athena queries to verify data accuracy and completeness.\n",
    "- Document steps, workflow diagrams, and data flow.\n",
    "- Prepare a short demo showing the results and key insights.\n",
    "- Review IAM roles and optimize permissions where needed.\n",
    "\n",
    "**Deliverable:**\n",
    "Tested and documented data pipeline ready for demo and review."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
