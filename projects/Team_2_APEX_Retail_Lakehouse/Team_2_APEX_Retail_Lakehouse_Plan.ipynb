{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc02a89f",
   "metadata": {},
   "source": [
    "# üß© Team 2 ‚Äì APEX  \n",
    "## Retail Lakehouse ‚Äì Implementation Plan\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Overview\n",
    "This project involves building a **Lakehouse architecture** for e-commerce transactions on AWS using **Delta Lake**.  \n",
    "The goal is to integrate data lake scalability and warehouse reliability using Spark, AWS Glue, and Athena ‚Äî enabling transactional consistency, schema evolution, and simplified ingestion pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Tech Stack Used\n",
    "- **Apache Spark (PySpark)** ‚Äì Core data processing engine  \n",
    "- **AWS Glue (3.0)** ‚Äì ETL orchestration for Spark jobs and Delta table creation  \n",
    "- **AWS S3** ‚Äì Storage for raw, archive, and Lakehouse zones  \n",
    "- **AWS Athena** ‚Äì SQL querying on Delta Lake data  \n",
    "- **AWS Redshift** ‚Äì Analytical querying and integration  \n",
    "- **Docker** ‚Äì Local testing environment for Spark-Delta integration  \n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Datasets\n",
    "- **products.csv** ‚Äì Product catalog data  \n",
    "- **orders.csv** ‚Äì Order-level transaction data  \n",
    "- **order_items.csv** ‚Äì Line-item level data per order  \n",
    "\n",
    "These files are stored in S3‚Äôs **raw zone**, organized by date folders (`2024-06-06`, `2024-06-07`).\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Step-by-Step Plan\n",
    "\n",
    "### 1Ô∏è‚É£ Lakehouse and Delta Overview\n",
    "- A **Lakehouse** blends a data lake‚Äôs scalability with a data warehouse‚Äôs structure.  \n",
    "- It supports **ACID transactions**, **time travel**, and **schema evolution**.  \n",
    "- Delta Lake acts as a **reliable storage layer** for Spark, simplifying ingestion and updates.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Local Spark and Docker Setup\n",
    "- Build and test the Spark job locally before AWS deployment.  \n",
    "- Dockerfile uses base image `amazon/aws-glue-libs:glue_libs_3.0`.  \n",
    "- Install required libraries:\n",
    "  ```bash\n",
    "  pip install pyspark delta-spark\n",
    "  ```\n",
    "- Mount the `delta-core.jar` file inside the container for Delta support.  \n",
    "- Commands:\n",
    "  ```bash\n",
    "  docker build -t spark-delta-lake-job .\n",
    "  docker run -v $(pwd):/app spark-delta-lake-job\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Spark Job Details\n",
    "- Reads raw CSV data from S3 `raw/` directory.  \n",
    "- Defines **primary keys**:\n",
    "  - `orders` ‚Üí `order_id`\n",
    "  - `order_items` ‚Üí `order_id`, `id`\n",
    "  - `products` ‚Üí `product_id`\n",
    "- Converts timestamps to `order_date` (partition key).  \n",
    "- Implements logic:\n",
    "  - If Delta Table exists ‚Üí Perform **upsert (merge)** using Delta APIs.  \n",
    "  - Else ‚Üí Write new Delta table to S3 (`df.write.format(\"delta\")`).  \n",
    "- Moves processed files from `raw/` ‚Üí `archive/` to prevent reprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ AWS Glue Job Deployment\n",
    "- Upload script (`spark_transactional_delta_lake.py`) and JAR to `s3://<bucket>/jars/`.  \n",
    "- Create Glue job:\n",
    "  - **Version:** 3.0  \n",
    "  - **IAM Role:** Custom Glue Role (S3 + CloudWatch access)  \n",
    "  - **Workers:** 2  \n",
    "  - **Timeout:** 10 minutes  \n",
    "  - **Max Concurrency:** 3  \n",
    "- Add job parameters:\n",
    "  ```bash\n",
    "  --table_name orders\n",
    "  --table_name order_items\n",
    "  --table_name products\n",
    "  ```\n",
    "- Run jobs sequentially and validate outputs in `lakehouse_dw/` folder.\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ Output Verification in S3\n",
    "- Confirm directories created in S3:\n",
    "  - `lakehouse_dw/orders/` ‚Äì partitioned by `order_date`  \n",
    "  - `archive/orders/` ‚Äì contains processed files  \n",
    "- Validate using Athena:\n",
    "  ```sql\n",
    "  SELECT * FROM orders LIMIT 10;\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 6Ô∏è‚É£ AWS Glue Crawlers and Athena\n",
    "- Create crawlers for each dataset (orders, order_items, products).  \n",
    "- Select data source: **Delta Lake**.  \n",
    "- Target path: `s3://<bucket>/lakehouse_dw/<dataset>/`  \n",
    "- Database: `supermarket_transactions_db`  \n",
    "- Run crawler ‚Üí Confirm tables created in Glue Data Catalog.  \n",
    "- Validate in Athena with queries:\n",
    "  ```sql\n",
    "  SELECT COUNT(*) AS total_orders, SUM(total_amount) AS total_revenue FROM orders;\n",
    "  ```\n",
    "- Demonstrate Delta Upsert by uploading `orders_updated.csv` to raw zone and re-running job.\n",
    "\n",
    "---\n",
    "\n",
    "### 7Ô∏è‚É£ Redshift Integration (Optional)\n",
    "- Connect Redshift ‚Üí Select **Federated User**.  \n",
    "- Access Glue Data Catalog database `supermarket_transactions_db`.  \n",
    "- Redshift cannot query Delta tables directly ‚Äî workaround via Athena:\n",
    "  ```sql\n",
    "  CREATE TABLE delta_lake_orders AS SELECT * FROM orders;\n",
    "  ```\n",
    "- Schedule this command daily to refresh the view for Redshift queries.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÜ Sprint Alignment\n",
    "\n",
    "| **Sprint** | **Duration** | **Focus Area** | **Key Deliverables** |\n",
    "|-------------|---------------|----------------|----------------------|\n",
    "| Sprint 1 | 16-Oct-2025 to 18-Oct-2025 (3 Days) | Local Spark Setup and Delta Testing | Docker validation, Delta JAR configuration |\n",
    "| Sprint 2 | 23-Oct-2025 to 27-Oct-2025 (4 Days) | Glue Job Deployment & S3 Validation | Glue job success, Archive validation |\n",
    "| Sprint 3 | 28-Oct-2025 to 31-Oct-2025 (4 Days) | Crawlers, Athena & Redshift Integration | Upsert testing, Query validation |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
