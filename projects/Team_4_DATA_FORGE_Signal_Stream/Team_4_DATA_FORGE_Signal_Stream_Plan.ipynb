{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "698e25f4",
   "metadata": {},
   "source": [
    "# üß© Team 4 ‚Äì DATA FORGE  \n",
    "## Signal Stream ‚Äì Real-Time Data Transformation & Streaming Dashboard\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Overview\n",
    "This project demonstrates **real-time data transformation** using **Spark Streaming on AWS Glue**, ingesting from **AWS Kinesis Data Streams**, transforming and writing to **S3**, and visualizing insights through a **Streamlit dashboard deployed on ECS**.  \n",
    "The workflow simulates a telecommunications company analyzing network metrics like signal strength, GPS precision, and network status in near real-time.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Tech Stack Used\n",
    "- **AWS Kinesis Data Streams** ‚Äì Real-time ingestion source  \n",
    "- **AWS Glue (Spark Streaming)** ‚Äì Real-time data transformation engine  \n",
    "- **AWS S3** ‚Äì Data Lake for storing transformed metrics  \n",
    "- **AWS Glue Crawlers** ‚Äì Automatic schema detection and catalog creation  \n",
    "- **Amazon Athena** ‚Äì SQL-based querying over S3 data  \n",
    "- **Streamlit** ‚Äì Real-time visualization layer  \n",
    "- **Amazon ECS (Fargate)** ‚Äì Containerized dashboard deployment  \n",
    "- **Docker** ‚Äì Local testing and containerization  \n",
    "- **IAM & CloudWatch** ‚Äì Security, monitoring, and logging\n",
    "\n",
    "---\n",
    "\n",
    "## üóÉ Datasets & Data Flow\n",
    "- **Source Data:** `mobile_logs.csv` containing signal, network, GPS, operator, and timestamp data.  \n",
    "- **Streaming Source:** AWS Kinesis Stream ‚Üí `mobile_coverage_logs`  \n",
    "- **Intermediate Storage:** Transformed data written to S3 partitions as Snappy Parquet files.  \n",
    "- **Analytics Layer:** Glue Crawlers create tables in Data Catalog ‚Üí queried via Athena.  \n",
    "- **Dashboard:** Streamlit app reads from Athena ‚Üí hosted on ECS Fargate.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Step-by-Step Plan\n",
    "\n",
    "### 1Ô∏è‚É£ Spark Streaming Job on AWS Glue\n",
    "- **Script:** Spark Streaming job reads from **Kinesis Data Stream (Mobile Coverage Logs)** using `glueContext.create_data_frame_from_options`.  \n",
    "- Defines schema using **StructType** and **StructField** for ~15 attributes.  \n",
    "- Reads JSON payloads ‚Üí applies schema ‚Üí converts timestamps ‚Üí extracts `hour` as partition key.  \n",
    "- Implements **watermarking (10 mins)** to handle late-arriving data safely.  \n",
    "- Performs real-time aggregations:\n",
    "  - Average Signal Strength by Operator  \n",
    "  - Average GPS Precision by Provider  \n",
    "  - Count of Network Status by Postal Code  \n",
    "- Writes partitioned outputs to S3 using `writeStream()` every 20 seconds (`trigger(ProcessingTime=\"20 seconds\")`).  \n",
    "- Output folders on S3:  \n",
    "  - `/aggregations/signal_strength_by_operator/`  \n",
    "  - `/aggregations/gps_precision_by_provider/`  \n",
    "  - `/aggregations/status_count/`  \n",
    "\n",
    "#### Deployment on AWS Glue\n",
    "- Job Name: `etl_streaming_mobile_logs`  \n",
    "- Engine: Spark Streaming | Version: Glue 4.0 | Workers: 2  \n",
    "- IAM Role: Custom Glue Role with permissions ‚Äì `AmazonS3FullAccess`, `AmazonKinesisFullAccess`, `CloudWatchLogsFullAccess`  \n",
    "- Run continuously (no timeout).\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Glue Crawlers & Athena Configuration\n",
    "- Create 3 crawlers to catalog transformed data:  \n",
    "  1. **crawler_gps_precision_by_provider**  \n",
    "  2. **crawler_signal_strength_by_operator**  \n",
    "  3. **crawler_status_count**\n",
    "- Target S3 paths corresponding to Spark output partitions.  \n",
    "- Database: `mobile_network_aggregations`  \n",
    "- Schedule: On-Demand  \n",
    "- Result: 3 tables auto-created in **Glue Data Catalog**.  \n",
    "- Validate via Athena queries:  \n",
    "  ```sql\n",
    "  SELECT * FROM gps_precision_by_provider LIMIT 10;\n",
    "  SELECT * FROM signal_strength_by_operator LIMIT 10;\n",
    "  SELECT * FROM status_count LIMIT 10;\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Streamlit App ‚Äì Local & Docker Deployment\n",
    "#### Local App Structure\n",
    "- Folder: `docker_streamlit/` containing:\n",
    "  - `app.py` ‚Äì Streamlit application logic.  \n",
    "  - `requirements.txt` ‚Äì Dependencies.  \n",
    "  - `Dockerfile` ‚Äì Container image definition.  \n",
    "- App connects to Athena and queries tables dynamically using Boto3 client.  \n",
    "- Displays metrics as auto-refreshing tables every 600 seconds (10 minutes).\n",
    "\n",
    "#### Example Code (app.py)\n",
    "```python\n",
    "def query_athena(client, query):\n",
    "    response = client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={'Database': 'mobile_network_aggregations'},\n",
    "        ResultConfiguration={'OutputLocation': 's3://<bucket>/athena_output/'}\n",
    "    )\n",
    "```\n",
    "#### Build & Run Locally\n",
    "```bash\n",
    "cd docker_streamlit\n",
    "docker build -t mobile_signal_app .\n",
    "docker run -p 8501:8501 -v ~/.aws:/root/.aws mobile_signal_app\n",
    "```\n",
    "- Access locally via: **http://localhost:8501**\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ ECR & ECS Deployment\n",
    "#### Step 1: Push Image to ECR\n",
    "1. Create repository: `streamlit_app`\n",
    "2. Authenticate and push:\n",
    "   ```bash\n",
    "   aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin <account>.dkr.ecr.us-east-1.amazonaws.com\n",
    "   docker tag mobile_signal_app <account>.dkr.ecr.us-east-1.amazonaws.com/streamlit_app:latest\n",
    "   docker push <account>.dkr.ecr.us-east-1.amazonaws.com/streamlit_app:latest\n",
    "   ```\n",
    "\n",
    "#### Step 2: Create ECS Cluster & Task Definition\n",
    "- **Cluster Name:** `streamlit_deployments`  \n",
    "- **Task Definition:** `streamlit_mobile_metrics`  \n",
    "- Launch Type: Fargate | OS: Linux ARM64  \n",
    "- Resources: 4 vCPU | 10 GB Memory  \n",
    "- IAM Role: `custom_streamlit_ecs_role` with:\n",
    "  - `AmazonAthenaFullAccess`\n",
    "  - `AmazonS3FullAccess`\n",
    "  - `CloudWatchLogsFullAccess`\n",
    "- Container Name: `streamlit_mobile_container`\n",
    "- Image URI: `<ECR repo>/streamlit_app:latest`\n",
    "- Port Mapping: 8501\n",
    "\n",
    "#### Step 3: Run Task\n",
    "- Deploy via ECS Console ‚Üí select cluster ‚Üí **Run Task**\n",
    "- Once `Running`, open **Logs ‚Üí External URL** ‚Üí launch Streamlit dashboard.\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ Validation\n",
    "1. Ingest data via Kinesis producer script (`kinesis_producer.py`) locally:\n",
    "   ```bash\n",
    "   python3 kinesis_producer.py\n",
    "   ```\n",
    "2. Verify Spark job status (running).  \n",
    "3. Monitor real-time updates in Streamlit dashboard hosted on ECS.  \n",
    "4. Data auto-refreshes every 600 seconds with latest Athena query results.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÜ Sprint Alignment\n",
    "\n",
    "| **Sprint** | **Duration** | **Focus Area** | **Key Deliverables** |\n",
    "|-------------|---------------|----------------|----------------------|\n",
    "| Sprint 1 | 16-Oct-2025 to 18-Oct-2025 (3 Days) | Spark Streaming Setup & Kinesis Integration | Kinesis Stream, Glue Streaming Job, Schema Validation |\n",
    "| Sprint 2 | 23-Oct-2025 to 27-Oct-2025 (4 Days) | Crawlers, Athena & Streamlit App | Glue Crawlers, Data Catalog, Streamlit Local App |\n",
    "| Sprint 3 | 28-Oct-2025 to 31-Oct-2025 (4 Days) | ECS Deployment & Dashboard Validation | ECS Deployment, Dashboard Auto-Refresh Integration |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
