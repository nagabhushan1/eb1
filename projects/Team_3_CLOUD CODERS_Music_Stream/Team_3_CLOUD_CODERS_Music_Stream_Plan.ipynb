{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aab2b267",
   "metadata": {},
   "source": [
    "# üß© Team 3 ‚Äì CLOUD CODERS  \n",
    "## Music Stream ‚Äì AWS Glue + Airflow Orchestration Plan\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Overview\n",
    "This project focuses on using **Airflow for orchestration** and **AWS Glue (Spark + Python Shell)** for data transformation, with **DynamoDB** as the target NoSQL database.  \n",
    "It demonstrates how Airflow can coordinate data validation, transformation, and ingestion workflows efficiently without handling heavy computation directly.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Tech Stack Used\n",
    "- **Apache Airflow** ‚Äì Orchestration and workflow scheduling  \n",
    "- **AWS Glue (Spark & Python Shell)** ‚Äì Data transformation and ingestion  \n",
    "- **AWS S3** ‚Äì Storage for raw, processed, and archived data  \n",
    "- **AWS DynamoDB** ‚Äì Target NoSQL database for metrics storage  \n",
    "- **Docker** ‚Äì Local development for Spark testing  \n",
    "- **IAM & CloudWatch** ‚Äì Security and logging\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Datasets\n",
    "- **songs.csv** ‚Äì Song catalog data  \n",
    "- **streams.csv** ‚Äì User stream data (listens, duration, etc.)  \n",
    "- **users.csv** ‚Äì User profile and demographic data  \n",
    "\n",
    "These files are placed under `s3://<bucket>/spotify_data/` with subfolders:  \n",
    "`songs/`, `user_streams/`, and `users/`.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Step-by-Step Plan\n",
    "\n",
    "### 1Ô∏è‚É£ Airflow for Orchestration\n",
    "- Airflow DAG (`dag_glue_workflow.py`) runs every **5 minutes**.  \n",
    "- DAG Structure:\n",
    "  1. **Check Files Task** ‚Äì Verifies input files (songs, users, streams) exist in S3.  \n",
    "  2. **Trigger Glue Spark Job** ‚Äì If valid, triggers Spark job for metric calculation.  \n",
    "  3. **Wait for Spark Completion** ‚Äì Ensures ETL is finished before proceeding.  \n",
    "  4. **Trigger Glue Python Shell Job** ‚Äì Loads metrics into DynamoDB.  \n",
    "  5. **Archive Task** ‚Äì Moves processed files from `/user_streams/` to `/archive/`.\n",
    "- If any required files are missing, DAG calls a **Skip Execution** dummy operator.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Spark Job ‚Äì Calculate Metrics ETL\n",
    "**Job Name:** `calculate_metrics_etl`  \n",
    "- **Purpose:** Aggregates listening metrics from streams and songs data.  \n",
    "- Reads data from S3 (`spotify_data/songs/` and `spotify_data/user_streams/`).  \n",
    "- Performs:  \n",
    "  - Typecasting and null removal on `track_id`.  \n",
    "  - Aggregation by `song` and `report_date` to compute:  \n",
    "    - Total Listens  \n",
    "    - Unique Users  \n",
    "    - Total Listening Time  \n",
    "    - Average Listening Time per User  \n",
    "- Joins aggregated data with song metadata.  \n",
    "- Uses **Window Functions** (`rank`, `partitionBy`, `orderBy`) to:  \n",
    "  - Get **Top 3 Songs per Genre per Day**.  \n",
    "  - Get **Top 5 Genres** by total listens.  \n",
    "- Writes final results to `s3://<bucket>/spotify_data/output/song_kpis/`.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Python Shell Job ‚Äì Insert Metrics Dynamo\n",
    "**Job Name:** `insert_metrics_dynamo`  \n",
    "- Reads Spark output (`song_kpis/`) from S3.  \n",
    "- Inserts or updates records into **DynamoDB table** `track_level_reports`.  \n",
    "- Uses **Upsert (UpdateItem)** operation based on composite key:  \n",
    "  - `track_id` (Partition Key)  \n",
    "  - `report_date` (Sort Key)  \n",
    "- Updates existing records with latest metrics if key exists, else inserts a new record.\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ Local Testing with Docker\n",
    "- Run Glue Spark job locally before AWS deployment using Glue Docker image:\n",
    "  ```bash\n",
    "  docker run -it -v ~/.aws:/home/glue_user/.aws   -v $(pwd):/home/glue_user/workspace   -e AWS_PROFILE=default   -p 4040:4040 -p 18080:18080   --name glue_spark_submit   amazon/aws-glue-libs:glue_libs_4.0.0   spark-submit glue_pyspark.py\n",
    "  ```\n",
    "- Replace `df.show()` with `df.write()` for production runs.\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ AWS Glue Job Deployment\n",
    "- Create **IAM Role** with:  \n",
    "  - `AmazonS3FullAccess`  \n",
    "  - `AmazonDynamoDBFullAccess`  \n",
    "  - `CloudWatchLogsFullAccess`\n",
    "- Deploy both Glue jobs in console:  \n",
    "  - **Spark Job:** `calculate_metrics_etl` (2 workers, 15 min timeout)  \n",
    "  - **Python Job:** `insert_metrics_dynamo` (1/16 DPU, 20 min timeout)\n",
    "- Attach the IAM role created above to both jobs.\n",
    "\n",
    "---\n",
    "\n",
    "### 6Ô∏è‚É£ Airflow Setup and Permissions\n",
    "- Upload DAG to Airflow environment ‚Üí `/dags/dag_glue_workflow.py`  \n",
    "- Update Airflow‚Äôs **Execution Role** to include:  \n",
    "  - `AWSGlueConsoleFullAccess` (required to trigger Glue jobs)  \n",
    "- DAG checks input files and executes Glue jobs sequentially.\n",
    "\n",
    "---\n",
    "\n",
    "### 7Ô∏è‚É£ DynamoDB Table Configuration\n",
    "- **Table Name:** `track_level_reports`  \n",
    "- **Partition Key:** `track_id`  \n",
    "- **Sort Key:** `report_date`  \n",
    "- Default capacity mode and auto-scaling enabled.  \n",
    "- Used for storing song-level metrics per day.  \n",
    "\n",
    "---\n",
    "\n",
    "### 8Ô∏è‚É£ Validation and Execution\n",
    "1. Place sample data files in S3:  \n",
    "   - `/spotify_data/songs/sample_songs.csv`  \n",
    "   - `/spotify_data/user_streams/sample_streams.csv`  \n",
    "   - `/spotify_data/users/sample_users.csv`  \n",
    "2. Trigger DAG manually from Airflow UI.  \n",
    "3. Monitor Glue job execution logs in CloudWatch.  \n",
    "4. Verify final results:\n",
    "   - `s3://<bucket>/spotify_data/output/song_kpis/`  \n",
    "   - DynamoDB table `track_level_reports` using query:  \n",
    "     ```sql\n",
    "     SELECT * FROM track_level_reports;\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "## üìÜ Sprint Alignment\n",
    "\n",
    "| **Sprint** | **Duration** | **Focus Area** | **Key Deliverables** |\n",
    "|-------------|---------------|----------------|----------------------|\n",
    "| Sprint 1 | 16-Oct-2025 to 18-Oct-2025 (3 Days) | Airflow DAG & Local Spark Testing | DAG structure, local PySpark validation |\n",
    "| Sprint 2 | 23-Oct-2025 to 27-Oct-2025 (4 Days) | Glue Job Deployment & DynamoDB Integration | Spark + Python Glue Jobs working end-to-end |\n",
    "| Sprint 3 | 28-Oct-2025 to 31-Oct-2025 (4 Days) | Automation, Logging & Validation | Airflow to Glue orchestration validated, DynamoDB populated |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
