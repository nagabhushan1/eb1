# ğŸš— Team_1_CODE_FLUX â€“ Rental Data Pipeline

![Architecture Diagram](images/architecture.png)

## ğŸ“˜ Project Overview
This project implements a **big data processing pipeline** for a vehicle rental marketplace.  
It demonstrates how to design and orchestrate an end-to-end workflow using the AWS ecosystem â€” combining **data ingestion, transformation, and analytics** within a unified pipeline.

The system processes multiple datasets:
- **Vehicles** â€“ Details of all vehicles available for rent  
- **Users** â€“ Registered customers on the platform  
- **Locations** â€“ Master dataset containing pickup and drop-off points  
- **Rental Transactions** â€“ Records of all rental activity including start date, end date, vehicle ID, pickup/drop-off locations, and total amount

---

## ğŸ§© Objectives
To **aggregate and transform** raw data stored in Amazon S3 into meaningful metrics using Spark on AWS EMR, and make it **queryable via Athena**.  
The project showcases:
- Efficient data storage in S3 (raw â†’ processed zones)  
- Data processing using EMR and PySpark  
- Metadata management via AWS Glue Crawlers & Data Catalog  
- Interactive querying through Amazon Athena  
- Workflow orchestration using AWS Step Functions  

---

## âš™ï¸ Tech Stack
| Component | Purpose |
|------------|----------|
| **Amazon S3** | Storage layer & data lake for raw and processed data |
| **AWS EMR (PySpark)** | Executes Spark-based data transformation jobs |
| **AWS Glue Crawlers & Data Catalog** | Infers schema and registers tables |
| **Amazon Athena** | Enables SQL-based analysis over S3 data |
| **AWS Step Functions** | Orchestrates and automates the EMR workflow |

---

## ğŸ§  Workflow Summary
1. Upload all source datasets into the **S3 raw zone**.  
2. Use **Step Functions** to trigger Spark jobs on EMR.  
3. Spark transforms the data and writes results to the **processed zone** in S3.  
4. **Glue Crawlers** extract metadata from processed data and update the Data Catalog.  
5. **Athena** queries the data for business insights (rental trends, utilization, revenue, etc.).  
6. Step Functions automatically **terminate the EMR cluster** after successful execution.

---

## ğŸ—‚ï¸ Repository Structure
```
Team_1_CODE_FLUX_Rental_Data_Pipeline/

TBA
```

---

## ğŸ§© Sprint Overview
| Sprint | Focus Area | Key Deliverables |
|---------|-------------|------------------|
| **Sprint 1 â€“ AWS Setup & Initialization** | Configure S3 structure, upload datasets, and verify connectivity | S3 buckets created, raw data uploaded |
| **Sprint 2 â€“ Pipeline Development & Execution** | Develop and test PySpark jobs on EMR | Working Spark ETL jobs, Step Function flow |
| **Sprint 3 â€“ Integration, Testing & Demo** | Integrate Glue, Athena, and finalize automation | Complete end-to-end workflow with Athena queries |

---

## ğŸ“‹ JIRA Board
Project tracking and sprint progress are maintained in the dedicated JIRA board:  
ğŸ”— [Team_1_CODE_FLUX â€“ Rental Data Pipeline JIRA Board](https://nagabhushanm.atlassian.net/jira/software/projects/REN/boards/3)

---

## ğŸ§‘â€ğŸ’» Contributors
**Team 1 â€“ CODE FLUX**  
- Vijay Kumar E  
- Smita Sudhakar Hegde  
- Sakshath K Shetty  
- Mallika Shree K C  
- Gokul Raj S  

---

### ğŸ“¸ Image Placeholder
Add your architecture diagram or workflow visualization here:
```
images/architecture.png
```

---

### ğŸ Maintainer
**Repository Owner:** [@nagabhushan1](https://github.com/nagabhushan1)
