{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b86ae8",
   "metadata": {},
   "source": [
    "# üß© Team 1 ‚Äì CODE FLUX  \n",
    "## Rental Data Pipeline - Implementation Plan\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Overview\n",
    "This project focuses on building a **big data processing pipeline** for a rental vehicle marketplace using AWS services.  \n",
    "We will implement an **end-to-end Spark-based workflow** leveraging **Amazon EMR**, **AWS Glue**, **Athena**, and **Step Functions** to process, transform, and analyze vehicle rental data stored in **Amazon S3**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Tech Stack Used\n",
    "- **Amazon EMR** ‚Äì Managed Hadoop/Spark cluster for distributed data processing  \n",
    "- **Amazon S3** ‚Äì Storage layer and data lake for raw and transformed datasets  \n",
    "- **AWS Glue** ‚Äì Schema inference and cataloging using Crawlers and Data Catalog  \n",
    "- **AWS Athena** ‚Äì SQL-based query engine for S3 data  \n",
    "- **AWS Step Functions** ‚Äì Workflow orchestration for EMR cluster automation  \n",
    "- **Docker** ‚Äì Local Spark testing environment before EMR deployment  \n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Datasets\n",
    "All datasets reside in the **S3 raw zone**:\n",
    "1. `vehicles.csv` ‚Äì List of all available rental vehicles  \n",
    "2. `users.csv` ‚Äì Platform user data  \n",
    "3. `locations.csv` ‚Äì Master location reference data  \n",
    "4. `rental_transactions.csv` ‚Äì Transactional rental data with start/end dates, pickup/drop locations, vehicle IDs, and total amount  \n",
    "\n",
    "---\n",
    "\n",
    "## üß© Step-by-Step Plan\n",
    "\n",
    "### 1Ô∏è‚É£ EMR and Spark Introduction\n",
    "- Understand EMR as a managed platform for big data frameworks like Spark and Hadoop.  \n",
    "- Learn scaling, pricing, and IAM-based access control concepts.  \n",
    "- Recognize EMR‚Äôs integration with AWS services for secure, cost-effective processing.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Local Spark Job Development\n",
    "- Develop two Spark jobs locally using Docker for validation.  \n",
    "- **Spark Job 1** ‚Äì Uses `rental_transactions` and `users` datasets.  \n",
    "  - Convert rental start/end time to timestamps.  \n",
    "  - Derive a `duration` column.  \n",
    "  - Join users and transactions to calculate:\n",
    "    - Total transactions & revenue  \n",
    "    - Avg. transaction value  \n",
    "    - Max/min rental duration  \n",
    "    - User-level metrics (spending, revenue, etc.)  \n",
    "- **Spark Job 2** ‚Äì Uses `rental_transactions`, `locations`, and `vehicles`.  \n",
    "  - Perform multi-table joins.  \n",
    "  - Derive:\n",
    "    - Location-level metrics (revenue, unique vehicles, avg. transaction value)  \n",
    "    - Vehicle-type-level metrics (revenue, count, duration, etc.)  \n",
    "- Validate both Spark jobs locally by printing DataFrames (`.show()`) before EMR execution.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ EMR Cluster Setup and Job Execution\n",
    "- Create EMR Cluster (version 7.1.0, Spark 3.5.0).  \n",
    "- Configure IAM roles:\n",
    "  - `EMR_Service_Role`\n",
    "  - `EMR_EC2_Instance_Profile_Role`\n",
    "- Upload Spark scripts to S3 bucket path:  \n",
    "  `s3://<your-bucket>/spark-scripts/`  \n",
    "- Modify PySpark scripts:\n",
    "  - Uncomment S3 write commands  \n",
    "  - Comment out local `.show()`  \n",
    "- Add steps for both Spark jobs in EMR console:\n",
    "  - **Step 1** ‚Äì Execute `spark_job_1.py`  \n",
    "  - **Step 2** ‚Äì Execute `spark_job_2.py`  \n",
    "- Verify outputs in S3 folder `output/` containing:\n",
    "  - `location_performance_metrics`\n",
    "  - `transaction_metrics`\n",
    "  - `user_metrics`\n",
    "  - `vehicle_performance_metrics`\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ AWS Glue Crawlers and Athena Querying\n",
    "- Create four AWS Glue Crawlers (one per output dataset).  \n",
    "- Assign IAM role with:\n",
    "  - `AWSGlueServiceRole`  \n",
    "  - `CloudWatchLogsFullAccess`\n",
    "- Configure each crawler with source path (S3 output folders).  \n",
    "- Create a new database: `rental_vehicles_db`.  \n",
    "- Run crawlers and confirm table creation in the Glue Data Catalog.  \n",
    "- In Athena:\n",
    "  - Select `rental_vehicles_db`  \n",
    "  - Preview tables (`SELECT * FROM table LIMIT 10;`)  \n",
    "  - Run analytical queries for insights.\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ AWS Step Functions Workflow\n",
    "- Create JSON definition `stepfunctions_emr.json` with 4 states:\n",
    "  1. **Create EMR Cluster**  \n",
    "  2. **Execute Spark Job 1**  \n",
    "  3. **Execute Spark Job 2**  \n",
    "  4. **Terminate Cluster**  \n",
    "- Define transitions (`Next`, `Catch`) to handle job success/failure.  \n",
    "- IAM Role Setup:\n",
    "  - Create Step Functions Execution Role  \n",
    "  - Attach inline policy from `execution_policy_stepfunctions.json`  \n",
    "  - Include `CloudWatchLogsFullAccess` and `S3FullAccess`  \n",
    "- Deploy and monitor execution flow:  \n",
    "  - Validate EMR cluster creation ‚Üí Spark jobs ‚Üí termination.  \n",
    "\n",
    "---\n",
    "\n",
    "### 6Ô∏è‚É£ EMR Serverless Execution\n",
    "- Create an **EMR Serverless application** and launch it via EMR Studio.  \n",
    "- Create runtime IAM Role with trust policy (`emr_serverless_trust_policy.json`).  \n",
    "- Configure:\n",
    "  - Runtime Role ‚Üí `EMRServerlessRole`  \n",
    "  - Script location ‚Üí Spark job in S3  \n",
    "  - Application type ‚Üí Spark (v7.1.0)  \n",
    "- Submit job via ‚ÄúSubmit Batch Job Run.‚Äù  \n",
    "- Validate job success in under 2 minutes.  \n",
    "- Note: compare cost-effectiveness of provisioned EMR vs EMR Serverless.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÜ Sprint Alignment\n",
    "\n",
    "| **Sprint** | **Duration** | **Focus Area** | **Key Deliverables** |\n",
    "|-------------|---------------|----------------|----------------------|\n",
    "| Sprint 1 | 16-Oct-2025 to 18-Oct-2025  (3 Days) | Environment Setup & Local Validation | EMR Roles, Local Spark Validation |\n",
    "| Sprint 2 | 23-Oct-2025 to 27-Oct-2025  (4 Days) | EMR Execution & Glue Integration | Cluster Jobs, Output Validation, Crawlers Setup |\n",
    "| Sprint 3 | 28-Oct-2025 to 31-Oct-2025  (4 Days) | Step Functions & Serverless Execution | Workflow Automation, Final Testing |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
