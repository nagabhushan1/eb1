# AWS Data Engineering Track – Capgemini Training

This repository contains all learning materials, labs, and exercises developed for the **AWS Data Engineering Track** conducted for Capgemini learners.  
It is designed to help participants build end-to-end skills in data engineering, covering the full spectrum from SQL and Data Warehousing to Spark and AWS cloud environments.

---

## 📁 Repository Overview

The repository is organized into multiple folders, each serving a specific purpose in the learning journey:

- **labs/** – Contains guided hands-on lab exercises for each module (SQL, DWH, Python, Spark, AWS).  
- **assignments/** – Includes practice assignments aligned with lab topics for learner evaluation.  
- **datasets/** – Holds sample datasets and schema files used across labs and exercises.  
- **coding_exercises/** – Features Jupyter notebooks with coding challenges and practical exercises.  
- **question_bank/** – Compiles topic-wise interview questions and practice sets for revision.  
- **README.md** – Repository documentation file (this document).  

---

## 🚀 Learning Objectives

The course and repository aim to help learners:

- Understand the **data engineering lifecycle** on AWS.  
- Gain proficiency in **SQL, ETL design, and data modeling**.  
- Implement **Data Warehousing concepts** using practical examples.  
- Work with **PySpark and SparkSQL** for big data processing.  
- Explore **AWS services** relevant to data engineering such as S3, IAM, EC2, Glue, and Redshift.  

---

## 🧠 How to Use This Repository

1. **Start with labs** – Follow step-by-step guided notebooks in the `labs/` folder.  
2. **Complete assignments** – Attempt the corresponding tasks to reinforce your learning.  
3. **Use datasets** – Load and explore the sample data provided for exercises.  
4. **Practice coding** – Use the `coding_exercises/` notebooks for additional hands-on practice.  
5. **Revise with question bank** – Strengthen your conceptual and interview preparation.  

---

## 🛠️ Requirements

- Python 3.x  
- Jupyter Notebook / JupyterLab  
- Docker
- Apache Spark (local or cloud setup)  
- AWS Console access (for S3, IAM, EC2, Glue, Redshift labs)  
- SQL environment (Oracle, Oracle Apex, or Oracle Live SQL)
- Google Colab

---

## ⚠️ Disclaimer

> This repository is intended solely for **Capgemini AWS Data Engineering training**.  
> All materials are meant for **educational purposes only** and should **not be shared, redistributed, or published** without prior written permission from the author or training team.

---

**Author:** Nagabhushan Mamadapur  
**Program:** AWS Data Engineering – Capgemini Bootcamp  
