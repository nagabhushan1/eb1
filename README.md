# AWS Data Engineering Track â€“ Capgemini Training

This repository contains all learning materials, labs, and exercises developed for the **AWS Data Engineering Track** conducted for Capgemini learners.  
It is designed to help participants build end-to-end skills in data engineering, covering the full spectrum from SQL and Data Warehousing to Spark and AWS cloud environments.

---

## ðŸ“ Repository Overview

The repository is organized into multiple folders, each serving a specific purpose in the learning journey:

- **labs/** â€“ Contains guided hands-on lab exercises for each module (SQL, DWH, Python, Spark, AWS).  
- **assignments/** â€“ Includes practice assignments aligned with lab topics for learner evaluation.  
- **datasets/** â€“ Holds sample datasets and schema files used across labs and exercises.  
- **coding_exercises/** â€“ Features Jupyter notebooks with coding challenges and practical exercises.  
- **question_bank/** â€“ Compiles topic-wise interview questions and practice sets for revision.  
- **README.md** â€“ Repository documentation file (this document).  

---

## ðŸš€ Learning Objectives

The course and repository aim to help learners:

- Understand the **data engineering lifecycle** on AWS.  
- Gain proficiency in **SQL, ETL design, and data modeling**.  
- Implement **Data Warehousing concepts** using practical examples.  
- Work with **PySpark and SparkSQL** for big data processing.  
- Explore **AWS services** relevant to data engineering such as S3, IAM, EC2, Glue, and Redshift.  

---

## ðŸ§  How to Use This Repository

1. **Start with labs** â€“ Follow step-by-step guided notebooks in the `labs/` folder.  
2. **Complete assignments** â€“ Attempt the corresponding tasks to reinforce your learning.  
3. **Use datasets** â€“ Load and explore the sample data provided for exercises.  
4. **Practice coding** â€“ Use the `coding_exercises/` notebooks for additional hands-on practice.  
5. **Revise with question bank** â€“ Strengthen your conceptual and interview preparation.  

---

## ðŸ› ï¸ Requirements

- Python 3.x  
- Jupyter Notebook / JupyterLab  
- Docker
- Apache Spark (local or cloud setup)  
- AWS Console access (for S3, IAM, EC2, Glue, Redshift labs)  
- SQL environment (Oracle, Oracle Apex, or Oracle Live SQL)
- Google Colab

---

## âš ï¸ Disclaimer

> This repository is intended solely for **Capgemini AWS Data Engineering training**.  
> All materials are meant for **educational purposes only** and should **not be shared, redistributed, or published** without prior written permission from the author or training team.

---

**Author:** Nagabhushan Mamadapur  
**Program:** AWS Data Engineering â€“ Capgemini Bootcamp  
