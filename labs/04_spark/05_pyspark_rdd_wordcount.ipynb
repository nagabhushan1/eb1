{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2b4bf3",
   "metadata": {},
   "source": [
    "\n",
    "## Objective\n",
    "This notebook demonstrates how to implement the **WordCount** program using **PySpark RDDs (Resilient Distributed Datasets)**.\n",
    "\n",
    "The **WordCount** example is a classic way to understand distributed data processing in Spark — reading text, transforming data, and performing aggregation.\n",
    "\n",
    "---\n",
    "\n",
    "## Concept Overview\n",
    "**WordCount** involves reading text data, splitting lines into words, and counting how many times each word appears.\n",
    "\n",
    "### Steps:\n",
    "1. Initialize a Spark Session  \n",
    "2. Load the text file into an RDD  \n",
    "3. Split lines into words using `flatMap()`  \n",
    "4. Map words to `(word, 1)` pairs using `map()`  \n",
    "5. Aggregate word counts using `reduceByKey()`  \n",
    "6. Collect and display results  \n",
    "\n",
    "| Transformation | Description |\n",
    "|----------------|--------------|\n",
    "| `textFile()` | Reads file into an RDD |\n",
    "| `flatMap()` | Splits lines into individual words |\n",
    "| `map()` | Creates key-value pairs `(word, 1)` |\n",
    "| `reduceByKey()` | Aggregates values by key |\n",
    "| `collect()` | Returns results to driver |\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use `flatMap()`?\n",
    "- `map()` → produces a list of lists (nested structure)\n",
    "- `flatMap()` → flattens and returns one continuous list of words\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7691e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e77b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file into an RDD\n",
    "filePath = 'samplefile.txt'\n",
    "linesRdd = spark.sparkContext.textFile(filePath)\n",
    "\n",
    "# Display the contents of RDD\n",
    "linesRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each line into words using flatMap()\n",
    "wordsRdd = linesRdd.flatMap(lambda x: x.split())\n",
    "wordsRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ff4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each word to a key-value pair (word, 1)\n",
    "wordsMapRdd = wordsRdd.map(lambda x: (x, 1))\n",
    "wordsMapRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce by key to count occurrences of each word\n",
    "wordCountRdd = wordsMapRdd.reduceByKey(lambda x, y: x + y)\n",
    "wordCountRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31cb920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all transformations into a single chain\n",
    "spark.sparkContext.textFile('samplefile.txt') \\    \n",
    ".flatMap(lambda x: x.split()) \\    \n",
    ".map(lambda x: (x, 1)) \\    \n",
    ".reduceByKey(lambda x, y: x + y) \\    \n",
    ".collect()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
