{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af72c337",
   "metadata": {},
   "source": [
    "\n",
    "# PySpark Airline Flight Analysis\n",
    "\n",
    "This notebook demonstrates how to work with **Airlines, Airports, and Flights datasets** using **PySpark RDDs**.  \n",
    "We will explore how to load data from CSV files, filter headers, parse structured data, and perform basic analytics.\n",
    "\n",
    "---\n",
    "\n",
    "### **Objectives**\n",
    "- Load CSV data into RDDs using Spark.\n",
    "- Remove header rows from datasets.\n",
    "- Parse date, time, and numeric fields.\n",
    "- Compute flight-level statistics using RDD transformations and actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0bf2f8",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Initialize Spark Session and Load Data\n",
    "\n",
    "We start by creating a `SparkSession` and reading the CSV files as text into RDDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c56744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "airlinesDataPath = 'airlines.csv'\n",
    "airportsDataPath = 'airports.csv'\n",
    "flightsDataPath = 'flights.csv'\n",
    "\n",
    "airlinesRdd = spark.sparkContext.textFile(airlinesDataPath)\n",
    "\n",
    "for i in airlinesRdd.take(10): \n",
    "    print(i)\n",
    "\n",
    "airlinesRdd.first()\n",
    "airlinesRdd.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f20ae2a",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Remove Header Rows\n",
    "\n",
    "CSV files often contain a header line that should not be part of computations.  \n",
    "We can filter it out using either a **lambda expression** or a **custom function**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c71483",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove header using a lambda expression\n",
    "airlinesWOHeaderRdd = airlinesRdd.filter(lambda x: 'Description' not in x)\n",
    "airlinesWOHeaderRdd.count()\n",
    "airlinesWOHeaderRdd.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d918f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function instead of a lambda expression\n",
    "def notHeader(row):\n",
    "    return 'Description' not in row\n",
    "\n",
    "airlinesWOHeaderRdd1 = airlinesRdd.filter(notHeader)\n",
    "airlinesWOHeaderRdd1.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e539a5",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Parsing Flight Data\n",
    "\n",
    "We’ll now parse flight records to extract useful fields such as dates, times, and distances.  \n",
    "To represent structured records, we use Python’s `namedtuple` along with datetime parsing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5638b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flightsRdd = spark.sparkContext.textFile(flightsDataPath)\n",
    "\n",
    "flightsRdd.first()\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = ('date', 'airline', 'flightnum', 'origin', 'dest', \n",
    "          'dep', 'dep_delay', 'arv', 'arv_delay', 'airtime', 'distance')\n",
    "\n",
    "Flight = namedtuple('Flight', fields, verbose=True)\n",
    "\n",
    "DATE_FMT = '%Y-%m-%d'\n",
    "TIME_FMT = '%H%M'\n",
    "\n",
    "def parse(row):\n",
    "    row[0] = datetime.strptime(row[0], DATE_FMT).date()\n",
    "    row[5] = datetime.strptime(row[5], TIME_FMT).time()\n",
    "    row[6] = float(row[6])\n",
    "    row[7] = datetime.strptime(row[7], TIME_FMT).time()\n",
    "    row[8] = float(row[8])\n",
    "    row[9] = float(row[9])\n",
    "    row[10] = float(row[10])\n",
    "    return Flight(*row[:11])\n",
    "\n",
    "flightsParsedRdd = flightsRdd.map(lambda x: x.split(\",\")).map(parse)\n",
    "flightsParsedRdd.first()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f089ff0",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Flight-Level Analysis\n",
    "\n",
    "We can perform aggregate computations such as calculating **average distance** and **percentage of delayed flights** using RDD actions like `reduce` and `filter`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbcc4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Average distance travelled by a flight\n",
    "totalDistance = flightsParsedRdd.map(lambda x: x.distance).reduce(lambda x, y: x + y)\n",
    "avgDistance = totalDistance / flightsParsedRdd.count()\n",
    "avgDistance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f45344",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Percentage of flights with delays\n",
    "flightsParsedRdd.filter(lambda x: x.dep_delay > 0).count() / float(flightsParsedRdd.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c4740e",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Summary\n",
    "\n",
    "In this notebook, we learned how to:\n",
    "- Load and preprocess CSV data using PySpark RDDs.  \n",
    "- Remove headers using filters and functions.  \n",
    "- Parse structured records using `namedtuple` and `datetime`.  \n",
    "- Perform analytical computations such as averages and ratios using RDD transformations and actions.\n",
    "\n",
    "This demonstrates how Spark RDDs can handle **large-scale flight data** with ease and flexibility.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
