{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d44fa87c",
   "metadata": {},
   "source": [
    "# ðŸ”¥ Introduction to Apache Spark\n",
    "Welcome to the world of **Apache Spark** â€” an open-source, distributed computing system that enables large-scale data processing and analytics. This notebook is designed for **beginners** and will help you understand Spark concepts step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d1d4eb",
   "metadata": {},
   "source": [
    "## What is Apache Spark?\n",
    "**Apache Spark** is an open-source, unified analytics engine for large-scale data processing. It provides high-level APIs in Python, Java, Scala, and R, and an optimized engine that supports general computation graphs.\n",
    "\n",
    "Simpler terms â€” Spark lets you process large datasets **in memory** across multiple machines efficiently. It was developed at **UC Berkeleyâ€™s AMPLab** and later became an Apache project.\n",
    "\n",
    "### Example:\n",
    "Imagine you have terabytes of log data stored across multiple servers. Spark helps you load, process, and analyze this data faster than traditional systems like Hadoop MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372acdeb",
   "metadata": {},
   "source": [
    "## Spark Research\n",
    "Apache Spark originated from research at the **AMPLab at UC Berkeley**. The main research paper titled *Resilient Distributed Datasets (RDDs): A Fault-Tolerant Abstraction for In-Memory Cluster Computing* introduced a new way to process large-scale data efficiently.\n",
    "\n",
    "The key research contribution was the **RDD (Resilient Distributed Dataset)** abstraction â€” a distributed collection of elements that can be operated on in parallel.\n",
    "\n",
    "### Reference:\n",
    "- Zaharia, M., Chowdhury, M., Das, T., et al. *Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing* (NSDI 2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3ef5d4",
   "metadata": {},
   "source": [
    "## Spark Timeline\n",
    "| Year | Milestone |\n",
    "|------|------------|\n",
    "| 2009 | Spark project started at UC Berkeley AMPLab |\n",
    "| 2010 | First research paper on RDDs published |\n",
    "| 2013 | Spark became an Apache Incubator project |\n",
    "| 2014 | Apache Spark 1.0 released |\n",
    "| 2015â€“2018 | Spark SQL, DataFrames, MLlib, and Streaming matured |\n",
    "| 2020+ | Spark 3.x introduced Adaptive Query Execution and better Python integration |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76b0e0",
   "metadata": {},
   "source": [
    "## Spark vs Hadoop\n",
    "| Feature | Apache Spark | Hadoop MapReduce |\n",
    "|----------|---------------|-----------------|\n",
    "| **Processing Model** | In-memory computation | Disk-based computation |\n",
    "| **Speed** | Up to 100x faster in memory | Slower due to disk I/O |\n",
    "| **Ease of Use** | APIs in Python, Scala, Java, R | Complex code in Java |\n",
    "| **Streaming** | Real-time (Spark Streaming) | Batch only |\n",
    "| **Fault Tolerance** | Through RDD lineage | Through data replication |\n",
    "| **Use Case** | Machine Learning, Streaming, ETL | Batch processing |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b10cd",
   "metadata": {},
   "source": [
    "## Working with Spark\n",
    "When you start using Spark, you typically create a **SparkSession** â€” your entry point to interact with all Spark features.\n",
    "\n",
    "Simplified workflow:\n",
    "1. Load data into Spark (from CSV, JSON, Parquet, etc.)\n",
    "2. Perform transformations (filter, map, join, etc.)\n",
    "3. Execute actions (collect, count, save, etc.)\n",
    "\n",
    "Spark works on top of a **cluster manager** (like YARN, Mesos, or Kubernetes) and runs computations in parallel across nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04428a44",
   "metadata": {},
   "source": [
    "## Spark RDDs (Resilient Distributed Datasets)\n",
    "**RDDs** are the core data structure of Spark. They are immutable, distributed collections of objects that can be processed in parallel.\n",
    "\n",
    "Each RDD is divided into **partitions**, and each partition is processed by one task on a cluster node.\n",
    "\n",
    "### Example:\n",
    "Loading a text file as an RDD (conceptually):\n",
    "```\n",
    "lines = sparkContext.textFile('data.txt')\n",
    "```\n",
    "Now you can perform transformations (like filtering lines containing a keyword) or actions (like counting total lines)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc55c20f",
   "metadata": {},
   "source": [
    "## RDD Operations\n",
    "There are two main types of operations on RDDs:\n",
    "\n",
    "1. **Transformations** â€“ Create a new RDD from an existing one (lazy).\n",
    "2. **Actions** â€“ Trigger computation and return a result to the driver program or write to storage.\n",
    "\n",
    "### Common Transformations and Actions\n",
    "| Type | Operation | Description |\n",
    "|------|------------|-------------|\n",
    "| **Transformation** | `map()` | Applies a function to each element and returns a new RDD |\n",
    "| **Transformation** | `filter()` | Returns elements that satisfy a condition |\n",
    "| **Transformation** | `flatMap()` | Similar to map but flattens the results |\n",
    "| **Transformation** | `union()` | Merges two RDDs |\n",
    "| **Transformation** | `distinct()` | Removes duplicates |\n",
    "| **Transformation** | `groupByKey()` | Groups data with the same key |\n",
    "| **Transformation** | `reduceByKey()` | Aggregates values for each key |\n",
    "| **Action** | `collect()` | Returns all elements to the driver |\n",
    "| **Action** | `count()` | Returns the number of elements |\n",
    "| **Action** | `first()` | Returns the first element |\n",
    "| **Action** | `take(n)` | Returns first n elements |\n",
    "| **Action** | `saveAsTextFile()` | Saves RDD content to external storage |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56158c7a",
   "metadata": {},
   "source": [
    "## Lazy Evaluation\n",
    "Spark does **not** execute transformations immediately. Instead, it builds a **DAG (Directed Acyclic Graph)** of transformations.\n",
    "\n",
    "The actual computation happens only when an **action** (like `count()` or `collect()`) is called.\n",
    "\n",
    "ðŸ’¡ **Example:**\n",
    "```\n",
    "rdd = sparkContext.textFile('data.txt')\n",
    "filtered = rdd.filter(lambda x: 'error' in x)\n",
    "filtered.count()  # Computation happens here!\n",
    "```\n",
    "This design optimizes execution by combining and reordering transformations efficiently before execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44508ea",
   "metadata": {},
   "source": [
    "## Passing Functions to Spark\n",
    "Spark allows you to pass **lambda functions** or named functions to transformations.\n",
    "\n",
    "Example (conceptual):\n",
    "```\n",
    "def is_even(x):\n",
    "    return x % 2 == 0\n",
    "\n",
    "numbers = sparkContext.parallelize([1,2,3,4,5,6])\n",
    "even_numbers = numbers.filter(is_even)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b65188",
   "metadata": {},
   "source": [
    "## Working with Key-Value Pairs\n",
    "Many operations in Spark use **pair RDDs** (key-value pairs).\n",
    "\n",
    "### Example:\n",
    "```\n",
    "data = [('A', 10), ('B', 20), ('A', 15)]\n",
    "rdd = sparkContext.parallelize(data)\n",
    "result = rdd.reduceByKey(lambda a, b: a + b)\n",
    "# Output â†’ ('A', 25), ('B', 20)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc59feb",
   "metadata": {},
   "source": [
    "## Shuffle Operations\n",
    "**Shuffle** is the process of redistributing data across partitions â€” often needed during operations like `groupByKey()` or `reduceByKey()`.\n",
    "\n",
    "Shuffles involve disk and network I/O, which makes them expensive.\n",
    "\n",
    "ðŸ’¡ **Tip:** Use `reduceByKey()` instead of `groupByKey()` to minimize shuffle size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d88e047",
   "metadata": {},
   "source": [
    "## RDD Persistence (Caching)\n",
    "By default, RDDs are recomputed every time you perform an action. To avoid this, Spark lets you **cache** or **persist** RDDs in memory.\n",
    "\n",
    "```\n",
    "rdd = sparkContext.textFile('data.txt')\n",
    "rdd.cache()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8778eed",
   "metadata": {},
   "source": [
    "## PySpark\n",
    "**PySpark** is the Python API for Apache Spark.\n",
    "\n",
    "It allows Python developers to leverage Sparkâ€™s distributed computation capabilities without writing Scala or Java code.\n",
    "\n",
    "PySpark provides modules for:\n",
    "- Spark SQL\n",
    "- DataFrames\n",
    "- Streaming\n",
    "- Machine Learning (MLlib)\n",
    "- Graph Processing (GraphX equivalent in Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928989e7",
   "metadata": {},
   "source": [
    "## Working with PySpark in Jupyter Notebooks (Interactive Environment)\n",
    "Jupyter Notebooks provide an **interactive environment** for running PySpark commands.\n",
    "\n",
    "### Steps:\n",
    "1. Install PySpark using pip:\n",
    "   ```bash\n",
    "   pip install pyspark\n",
    "   ```\n",
    "2. Launch Jupyter Notebook:\n",
    "   ```bash\n",
    "   jupyter notebook\n",
    "   ```\n",
    "3. Create a SparkSession inside the notebook:\n",
    "   ```python\n",
    "   from pyspark.sql import SparkSession\n",
    "   spark = SparkSession.builder.appName('MyApp').getOrCreate()\n",
    "   ```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
