{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319aa2ae",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§  Apache Server Log Analysis using PySpark RDD\n",
    "\n",
    "## Overview\n",
    "In this notebook, weâ€™ll use **PySpark RDDs** to analyze an **Apache web server log file** and identify **bad requests** (status codes `400`, `404`, and `500`).\n",
    "\n",
    "We'll follow these steps:\n",
    "1. Create a Spark session.  \n",
    "2. Read the log file into an RDD.  \n",
    "3. Explore a few records.  \n",
    "4. Extract HTTP status codes.  \n",
    "5. Filter only bad requests and count them.  \n",
    "6. Return the count of each bad request type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9933dacf",
   "metadata": {},
   "source": [
    "## Step 1: Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a380c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b06bf",
   "metadata": {},
   "source": [
    "## Step 2: Read the Apache Log File into an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed5c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logDataPath = 'access.log.1'\n",
    "logRdd = spark.sparkContext.textFile(logDataPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff5b30",
   "metadata": {},
   "source": [
    "## Step 3: Preview the First Log Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d283834",
   "metadata": {},
   "outputs": [],
   "source": [
    "logRdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07606d95",
   "metadata": {},
   "source": [
    "## Step 4: Inspect the Log Line Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b102bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = logRdd.first()\n",
    "x.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59efdc89",
   "metadata": {},
   "source": [
    "## Step 5: Extract the Status Code Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea200d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.split()[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda x: x.split()[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c6159",
   "metadata": {},
   "source": [
    "## Step 6: Filter Bad Requests and Count Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb288720",
   "metadata": {},
   "outputs": [],
   "source": [
    "logRdd.filter(lambda x: x.split()[8] in ['400', '404','500']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d03968",
   "metadata": {},
   "source": [
    "## Step 7: Count the Occurrences of Each Bad Request Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9efe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "logRdd.filter(lambda x: x.split()[8] in ['400', '404','500']) \\    .map(lambda x: (x.split()[8], 1)) \\    .reduceByKey(lambda x, y: x + y) \\    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea80c0",
   "metadata": {},
   "source": [
    "\n",
    "## âœ… Summary\n",
    "- We read Apache log data into an RDD.  \n",
    "- Explored log structure to identify the status code field.  \n",
    "- Filtered and counted bad requests (`400`, `404`, `500`).  \n",
    "- Aggregated counts per error code using `reduceByKey()`.\n",
    "\n",
    "This is a simple yet powerful example of **real-world log analysis using RDD transformations and actions** in PySpark.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
