{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4bea5f0",
   "metadata": {},
   "source": [
    "## Step 1: Discover all CSV files under `retail_data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825eb644",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "src_file_names = glob.glob('retail_data/**/*.csv', recursive=True)\n",
    "src_file_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34947b38",
   "metadata": {},
   "source": [
    "## Step 2: Extract dataset names from file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cbe269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def extract_dataset_names(file_list):\n",
    "    pattern = r\"retail_data/(\\w+)\\.csv\"\n",
    "    results = []\n",
    "    for f in file_list:\n",
    "        m = re.search(pattern, f)\n",
    "        if m:\n",
    "            dataset_name = m.group(1)\n",
    "            results.append((f, dataset_name))\n",
    "    return results\n",
    "\n",
    "datasets = extract_dataset_names(src_file_names)\n",
    "print(\"Datasets discovered:\")\n",
    "for f, n in datasets:\n",
    "    print(f\"{n} → {f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33753703",
   "metadata": {},
   "source": [
    "## Step 3: Load schema definitions from `retail_schema/schemas.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b6016",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "def load_schemas(schema_path='retail_schema/schemas.json'):\n",
    "    with open(schema_path, 'r') as f:\n",
    "        schemas = json.load(f)\n",
    "    print(\"Schemas loaded successfully.\")\n",
    "    return schemas\n",
    "\n",
    "schemas = load_schemas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ca602e",
   "metadata": {},
   "source": [
    "## Step 4: Read dataset with schema applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c765c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_dataset_with_schema(file_path, dataset_name, schemas):\n",
    "    schema = schemas.get(dataset_name)\n",
    "    if schema is None:\n",
    "        raise ValueError(f\"No schema found for dataset: {dataset_name}\")\n",
    "    \n",
    "    columns = sorted(schema, key=lambda x: x['column_position'])\n",
    "    col_names = [col['column_name'] for col in columns]\n",
    "    \n",
    "    dtype_map = {}\n",
    "    for col in schema:\n",
    "        dtype = col['data_type']\n",
    "        if dtype == 'integer':\n",
    "            dtype_map[col['column_name']] = 'Int64'\n",
    "        elif dtype == 'float':\n",
    "            dtype_map[col['column_name']] = 'float'\n",
    "        elif dtype == 'string' or dtype == '':\n",
    "            dtype_map[col['column_name']] = 'string'\n",
    "        elif dtype == 'timestamp':\n",
    "            dtype_map[col['column_name']] = 'string'\n",
    "        else:\n",
    "            dtype_map[col['column_name']] = 'string'\n",
    "\n",
    "    df = pd.read_csv(file_path, names=col_names, dtype=dtype_map, header=None)\n",
    "    \n",
    "    timestamp_cols = [c['column_name'] for c in schema if c['data_type'] == 'timestamp']\n",
    "    for col in timestamp_cols:\n",
    "        try:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce', infer_datetime_format=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46118cf7",
   "metadata": {},
   "source": [
    "## Step 5: Read all datasets and apply schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframes = {}\n",
    "\n",
    "for file_path, ds_name in datasets:\n",
    "    df = read_dataset_with_schema(file_path, ds_name, schemas)\n",
    "    dataframes[ds_name] = df\n",
    "    print(f\"Loaded {ds_name} → {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860148f1",
   "metadata": {},
   "source": [
    "## Step 6: Validate dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290f54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "expected = {\"categories\", \"customers\", \"departments\", \"order_items\", \"orders\", \"products\"}\n",
    "loaded = set(dataframes.keys())\n",
    "\n",
    "if expected == loaded:\n",
    "    print(\"All 6 datasets loaded successfully.\")\n",
    "else:\n",
    "    print(\"Some datasets missing.\")\n",
    "    print(\"Expected:\", expected)\n",
    "    print(\"Loaded:\", loaded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732473c8",
   "metadata": {},
   "source": [
    "## Step 7: Display sample records from each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba863fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n{name.upper()} SAMPLE:\")\n",
    "    display(df.head())\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
