{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "af12af5e", "cell_type": "markdown", "source": "# PySpark and Big Data Overview\n\n## Understanding Big Data\n\n**Big Data** refers to extremely large and complex datasets that cannot be handled by traditional data processing applications. These datasets come from various sources such as social media, sensors, transactions, and more.\n\n### The 4 Vs of Big Data\n\n| Dimension | Description |\n|------------|--------------|\n| **Volume** | Refers to the massive amount of data generated every second. Organizations deal with terabytes to petabytes of data. |\n| **Velocity** | Describes the speed at which new data is generated, collected, and processed \u2014 often in real-time. |\n| **Variety** | Represents the different types of data \u2014 structured, semi-structured, and unstructured (text, images, audio, etc.). |\n| **Veracity** | Refers to the trustworthiness and accuracy of the data collected. |\n\n### Data Size Units\n\n| Unit | Size |\n|------|------|\n| 1 Byte | 8 bits |\n| 1 Kilobyte (KB) | 1,024 Bytes |\n| 1 Megabyte (MB) | 1,024 KB |\n| 1 Gigabyte (GB) | 1,024 MB |\n| 1 Terabyte (TB) | 1,024 GB |\n| 1 Petabyte (PB) | 1,024 TB |\n| 1 Exabyte (EB) | 1,024 PB |\n| 1 Zettabyte (ZB) | 1,024 EB |\n| 1 Yottabyte (YB) | 1,024 ZB |\n\n### Value Proposition of Big Data\n\nBig Data allows organizations to:\n- Gain deeper business insights\n- Improve operational efficiency\n- Personalize customer experiences\n- Enable predictive analytics and real-time decision making\n\n---\n\n## Apache Spark in Big Data Processing\n\nApache Spark is an open-source **distributed data processing framework** designed for speed, scalability, and ease of use.\n\n### Key Features of Spark\n- **In-memory computation** for faster performance.\n- **Supports multiple languages** \u2014 Python (PySpark), Scala, Java, R, SQL.\n- **Rich APIs** for streaming, machine learning, and graph processing.\n- **Scales horizontally** across a cluster of machines.\n\n### Spark Ecosystem Components\n- **Spark Core** \u2013 The foundation, responsible for distributed computation.\n- **Spark SQL** \u2013 For structured data queries using SQL.\n- **Spark Streaming** \u2013 For real-time data processing.\n- **MLlib** \u2013 For machine learning operations.\n- **GraphX** \u2013 For graph processing.\n\n---\n\n## RDD \u2013 Resilient Distributed Dataset\n\nRDD is the **fundamental data structure in Spark**. It represents an **immutable distributed collection of objects** that can be processed in parallel.\n\n### Characteristics of RDDs\n- **Resilient** \u2013 Fault-tolerant with automatic recovery from failures.\n- **Distributed** \u2013 Data is divided across multiple nodes.\n- **Immutable** \u2013 Once created, cannot be modified.\n- **Lazy Evaluation** \u2013 Operations are not executed until an action is triggered.\n\n### RDD Operations\n\n1. **Transformations** \u2013 Return a new RDD from an existing one (lazy).\n   - Examples: `map()`, `filter()`, `flatMap()`, `distinct()`\n2. **Actions** \u2013 Trigger computation and return a result.\n   - Examples: `count()`, `collect()`, `first()`, `saveAsTextFile()`\n\n---\n", "metadata": {}}, {"id": "b8c4b464", "cell_type": "markdown", "source": "## Setting up Spark using Docker\n\nThe following steps explain how to set up **Apache Spark** using Docker.\n\n> \u2699\ufe0f **Note:** Docker must be installed on your machine before running these commands.\n\n### Step 1: Pull the Spark Docker Image\n```bash\ndocker pull jupyter/all-spark-notebook:95f855f8e55f\n```\n\n### Step 2: Run the Spark Container\n```bash\ndocker run -p 8888:8888 --name spark jupyter/all-spark-notebook:95f855f8e55f\n```\n\nThis command:\n- Maps port **8888** of the container to your local machine.\n- Starts a container named **spark**.\n- Launches Jupyter Notebook with Spark pre-configured.\n\nOnce the container is running, you can access Jupyter Notebook in your browser using the URL displayed in the terminal (typically `http://127.0.0.1:8888`).\n\n---\n", "metadata": {}}]}