{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419392b3",
   "metadata": {},
   "source": [
    "# Amazon Redshift Serverless with Boto3\n",
    "Region: **us-east-1**\n",
    "\n",
    "This notebook demonstrates how to interact with **Amazon Redshift Serverless** end-to-end using the **boto3 Redshift Data API**.\n",
    "\n",
    "While you will create your **namespace**, **workgroup**, and **IAM role** in the AWS Console, this notebook focuses on running SQL statements programmatically through the Data API.\n",
    "\n",
    "### Steps Covered\n",
    "1. Setup Redshift Data API client\n",
    "2. Create table for sample *tickit* dataset\n",
    "3. Load data from S3 into Redshift\n",
    "4. Run analytical queries\n",
    "5. Fetch and display results\n",
    "6. Optional: Unload data back to S3\n",
    "7. Cleanup resources\n",
    "\n",
    "---\n",
    "### Prerequisites\n",
    "- Redshift Serverless namespace and workgroup created via AWS Console\n",
    "- Workgroup has IAM role with S3 access for COPY/UNLOAD\n",
    "- Tickit dataset available in S3 (example: `s3://aws-tickit-data/`)\n",
    "- AWS credentials configured in local environment (`aws configure`)\n",
    "- Permissions: `AmazonRedshiftDataFullAccess`, `AmazonS3FullAccess`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c102f74",
   "metadata": {},
   "source": [
    "## Step 1: Setup Redshift Data API Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Replace with your configuration\n",
    "region = 'us-east-1'\n",
    "workgroup = 'my-redshift-workgroup'   # Replace with your Redshift Serverless workgroup\n",
    "database = 'dev'                      # Replace with your Redshift database\n",
    "schema = 'public'\n",
    "iam_role_arn = 'arn:aws:iam::123456789012:role/MyRedshiftS3AccessRole'\n",
    "s3_data_path = 's3://awssampledbuswest2/tickit/spectrum/sales/'   # Sample tickit data\n",
    "s3_unload_path = 's3://my-redshift-output-bucket/unload/'\n",
    "\n",
    "redshift_data = boto3.client('redshift-data', region_name=region)\n",
    "print('✅ Redshift Data API client initialized.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e4c8a8",
   "metadata": {},
   "source": [
    "## Step 2: Helper Functions for Query Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b589525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql(sql: str):\n",
    "    response = redshift_data.execute_statement(\n",
    "        WorkgroupName=workgroup,\n",
    "        Database=database,\n",
    "        Sql=sql\n",
    "    )\n",
    "    return response['Id']\n",
    "\n",
    "def wait_for_query(statement_id: str):\n",
    "    status = 'RUNNING'\n",
    "    while status in ['RUNNING', 'STARTED', 'SUBMITTED']:\n",
    "        result = redshift_data.describe_statement(Id=statement_id)\n",
    "        status = result['Status']\n",
    "        if status in ['FAILED', 'ABORTED']:\n",
    "            raise Exception(f\"Query {statement_id} failed: {result['Error']}\" )\n",
    "        time.sleep(2)\n",
    "    return result\n",
    "\n",
    "def fetch_results(statement_id: str):\n",
    "    result = redshift_data.get_statement_result(Id=statement_id)\n",
    "    cols = [col['name'] for col in result['ColumnMetadata']]\n",
    "    rows = []\n",
    "    for record in result['Records']:\n",
    "        rows.append([list(field.values())[0] if field else None for field in record])\n",
    "    return pd.DataFrame(rows, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049393dc",
   "metadata": {},
   "source": [
    "## Step 3: Create Tickit Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d2439",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {schema}.sales(\n",
    "    salesid INTEGER NOT NULL,\n",
    "    listid INTEGER NOT NULL,\n",
    "    sellerid INTEGER NOT NULL,\n",
    "    buyerid INTEGER NOT NULL,\n",
    "    eventid INTEGER NOT NULL,\n",
    "    dateid SMALLINT NOT NULL,\n",
    "    qtysold SMALLINT NOT NULL,\n",
    "    pricepaid DECIMAL(8,2),\n",
    "    commission DECIMAL(8,2),\n",
    "    saletime TIMESTAMP\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "stmt_id = execute_sql(create_table)\n",
    "wait_for_query(stmt_id)\n",
    "print('✅ sales table created successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c9ec01",
   "metadata": {},
   "source": [
    "## Step 4: Load Tickit Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7051e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_sql = f\"\"\"\n",
    "COPY {schema}.sales\n",
    "FROM '{s3_data_path}'\n",
    "IAM_ROLE '{iam_role_arn}'\n",
    "FORMAT AS PARQUET;\n",
    "\"\"\"\n",
    "\n",
    "stmt_id = execute_sql(copy_sql)\n",
    "wait_for_query(stmt_id)\n",
    "print('✅ Data loaded successfully from S3.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c1c4b",
   "metadata": {},
   "source": [
    "## Step 5: Run Analytical Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77773c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Total sales count\n",
    "sql = \"SELECT COUNT(*) AS total_sales FROM sales;\"\n",
    "stmt_id = execute_sql(sql)\n",
    "wait_for_query(stmt_id)\n",
    "df1 = fetch_results(stmt_id)\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: Top 5 events by total revenue\n",
    "sql = \"\"\"\n",
    "SELECT eventid, SUM(pricepaid) AS total_revenue\n",
    "FROM sales\n",
    "GROUP BY eventid\n",
    "ORDER BY total_revenue DESC\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "stmt_id = execute_sql(sql)\n",
    "wait_for_query(stmt_id)\n",
    "df2 = fetch_results(stmt_id)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c3dc2",
   "metadata": {},
   "source": [
    "## Step 6: Optional - Unload Query Results to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b3adca",
   "metadata": {},
   "outputs": [],
   "source": [
    "unload_sql = f\"\"\"\n",
    "UNLOAD ('SELECT * FROM sales WHERE qtysold > 10')\n",
    "TO '{s3_unload_path}'\n",
    "IAM_ROLE '{iam_role_arn}'\n",
    "PARQUET\n",
    "ALLOWOVERWRITE;\n",
    "\"\"\"\n",
    "\n",
    "stmt_id = execute_sql(unload_sql)\n",
    "wait_for_query(stmt_id)\n",
    "print('✅ Unload completed successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38a5e2",
   "metadata": {},
   "source": [
    "## Step 7: Cleanup Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ba7217",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_table = f\"DROP TABLE IF EXISTS {schema}.sales;\"\n",
    "stmt_id = execute_sql(drop_table)\n",
    "wait_for_query(stmt_id)\n",
    "print('✅ Table dropped successfully.')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
